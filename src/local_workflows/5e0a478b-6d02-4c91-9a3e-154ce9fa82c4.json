{
  "description": "What this workflow does\nThis workflows is a very simple workflow to use IPAdapter IP-Adapter is an effective and lightweight adapter to achieve image prompt capability for stable diffusion models.\n\nHow to use this workflow\nThe IPAdapter model has to match the CLIP vision encoder and of course the main checkpoint. All SD15 models and all models ending with \"vit-h\" use the SD15 CLIP vision. One of the SDXL models and all models ending with \"vit-g\" use the SDXL CLIP vision.\nPLUS models use more tokens and are stronger. LIGHT models have a very light impact. FACE and FULL-FACE are only to describe faces (they are not face swap!)",
  "name": "Basic IPAdapter",
  "stars": "3.0",
  "number_reviews": 3,
  "workflow_like": 50,
  "workflow_view": 55800,
  "workflow_download": 14000,
  "workflow_comment": 5,
  "author_workflow": 33,
  "author_download": 304400,
  "author_like": 936,
  "author_view": 966200,
  "url": "https://openart.ai/workflows/openart/-/8H0mGG5dOFQCuslp2qQv",
  "workflow_json": {
    "last_node_id": 17,
    "last_link_id": 22,
    "nodes": [
      {
        "id": 8,
        "type": "CLIPTextEncode",
        "pos": [650, 420],
        "size": {
          "0": 210,
          "1": 120
        },
        "flags": {},
        "order": 8,
        "mode": 0,
        "inputs": [
          {
            "name": "clip",
            "type": "CLIP",
            "link": 6
          }
        ],
        "outputs": [
          {
            "name": "CONDITIONING",
            "type": "CONDITIONING",
            "links": [9],
            "shape": 3,
            "slot_index": 0
          }
        ],
        "properties": {
          "Node name for S&R": "CLIPTextEncode"
        },
        "widgets_values": ["blurry, horror"],
        "color": "#322",
        "bgcolor": "#533"
      },
      {
        "id": 11,
        "type": "VAEDecode",
        "pos": [1300, 170],
        "size": {
          "0": 140,
          "1": 50
        },
        "flags": {},
        "order": 11,
        "mode": 0,
        "inputs": [
          {
            "name": "samples",
            "type": "LATENT",
            "link": 11
          },
          {
            "name": "vae",
            "type": "VAE",
            "link": 12
          }
        ],
        "outputs": [
          {
            "name": "IMAGE",
            "type": "IMAGE",
            "links": [13],
            "shape": 3,
            "slot_index": 0
          }
        ],
        "properties": {
          "Node name for S&R": "VAEDecode"
        },
        "color": "#323",
        "bgcolor": "#535"
      },
      {
        "id": 12,
        "type": "SaveImage",
        "pos": [1300, 270],
        "size": {
          "0": 400,
          "1": 450
        },
        "flags": {},
        "order": 12,
        "mode": 0,
        "inputs": [
          {
            "name": "images",
            "type": "IMAGE",
            "link": 13
          }
        ],
        "properties": {},
        "widgets_values": ["IPAdapter"]
      },
      {
        "id": 13,
        "type": "Note",
        "pos": [131, -270],
        "size": {
          "0": 456.8177490234375,
          "1": 273.5598449707031
        },
        "flags": {},
        "order": 0,
        "mode": 0,
        "properties": {
          "text": ""
        },
        "widgets_values": [
          "BASIC IPADAPTER\n===============\n\nBe careful when selecting the models!\n\nThe IPAdapter model has to match the CLIP vision encoder and of course the main checkpoint.\n\nAll SD15 models and all models ending with \"vit-h\" use the SD15 CLIP vision.\n\nOne of the SDXL models and all models ending with \"vit-g\" use the SDXL CLIP vision.\n\nPLUS models use more tokens and are stronger. LIGHT models have a very light impact. FACE and FULL-FACE are only to describe faces (they are not face swap!)"
        ],
        "color": "#432",
        "bgcolor": "#653"
      },
      {
        "id": 10,
        "type": "EmptyLatentImage",
        "pos": [650, 590],
        "size": {
          "0": 210,
          "1": 110
        },
        "flags": {},
        "order": 1,
        "mode": 0,
        "outputs": [
          {
            "name": "LATENT",
            "type": "LATENT",
            "links": [10],
            "shape": 3,
            "slot_index": 0
          }
        ],
        "properties": {
          "Node name for S&R": "EmptyLatentImage"
        },
        "widgets_values": [512, 512, 1],
        "color": "#323",
        "bgcolor": "#535"
      },
      {
        "id": 2,
        "type": "VAELoader",
        "pos": [933, 485],
        "size": {
          "0": 300,
          "1": 60
        },
        "flags": {},
        "order": 2,
        "mode": 0,
        "outputs": [
          {
            "name": "VAE",
            "type": "VAE",
            "links": [12],
            "shape": 3,
            "slot_index": 0
          }
        ],
        "properties": {
          "Node name for S&R": "VAELoader"
        },
        "widgets_values": ["vae-ft-mse-840000-ema-pruned.safetensors"],
        "color": "#223",
        "bgcolor": "#335"
      },
      {
        "id": 9,
        "type": "KSampler",
        "pos": [930, 170],
        "size": {
          "0": 315,
          "1": 262
        },
        "flags": {},
        "order": 10,
        "mode": 0,
        "inputs": [
          {
            "name": "model",
            "type": "MODEL",
            "link": 22
          },
          {
            "name": "positive",
            "type": "CONDITIONING",
            "link": 8
          },
          {
            "name": "negative",
            "type": "CONDITIONING",
            "link": 9
          },
          {
            "name": "latent_image",
            "type": "LATENT",
            "link": 10
          }
        ],
        "outputs": [
          {
            "name": "LATENT",
            "type": "LATENT",
            "links": [11],
            "shape": 3,
            "slot_index": 0
          }
        ],
        "properties": {
          "Node name for S&R": "KSampler"
        },
        "widgets_values": [
          733897465521530,
          "randomize",
          25,
          6,
          "ddim",
          "ddim_uniform",
          1
        ],
        "color": "#2a363b",
        "bgcolor": "#3f5159"
      },
      {
        "id": 7,
        "type": "CLIPTextEncode",
        "pos": [650, 250],
        "size": {
          "0": 210,
          "1": 120
        },
        "flags": {},
        "order": 7,
        "mode": 0,
        "inputs": [
          {
            "name": "clip",
            "type": "CLIP",
            "link": 5
          }
        ],
        "outputs": [
          {
            "name": "CONDITIONING",
            "type": "CONDITIONING",
            "links": [8],
            "shape": 3,
            "slot_index": 0
          }
        ],
        "properties": {
          "Node name for S&R": "CLIPTextEncode"
        },
        "widgets_values": ["beautiful renaissance girl, detailed"],
        "color": "#232",
        "bgcolor": "#353"
      },
      {
        "id": 3,
        "type": "IPAdapterModelLoader",
        "pos": [210, 60],
        "size": {
          "0": 370,
          "1": 60
        },
        "flags": {},
        "order": 3,
        "mode": 0,
        "outputs": [
          {
            "name": "IPADAPTER",
            "type": "IPADAPTER",
            "links": [18],
            "shape": 3,
            "slot_index": 0
          }
        ],
        "properties": {
          "Node name for S&R": "IPAdapterModelLoader"
        },
        "widgets_values": ["ip-adapter_sd15.safetensors"],
        "color": "#223",
        "bgcolor": "#335"
      },
      {
        "id": 1,
        "type": "CheckpointLoaderSimple",
        "pos": [210, 280],
        "size": {
          "0": 370,
          "1": 110
        },
        "flags": {},
        "order": 4,
        "mode": 0,
        "outputs": [
          {
            "name": "MODEL",
            "type": "MODEL",
            "links": [20],
            "shape": 3,
            "slot_index": 0
          },
          {
            "name": "CLIP",
            "type": "CLIP",
            "links": [5, 6],
            "shape": 3,
            "slot_index": 1
          },
          {
            "name": "VAE",
            "type": "VAE",
            "links": null,
            "shape": 3
          }
        ],
        "properties": {
          "Node name for S&R": "CheckpointLoaderSimple"
        },
        "widgets_values": ["dreamshaper_8.safetensors"],
        "color": "#223",
        "bgcolor": "#335"
      },
      {
        "id": 4,
        "type": "CLIPVisionLoader",
        "pos": [210, 170],
        "size": {
          "0": 370,
          "1": 60
        },
        "flags": {},
        "order": 5,
        "mode": 0,
        "outputs": [
          {
            "name": "CLIP_VISION",
            "type": "CLIP_VISION",
            "links": [19],
            "shape": 3,
            "slot_index": 0
          }
        ],
        "properties": {
          "Node name for S&R": "CLIPVisionLoader"
        },
        "widgets_values": ["SD1.5/CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"],
        "color": "#223",
        "bgcolor": "#335"
      },
      {
        "id": 6,
        "type": "LoadImage",
        "pos": [-62, 69],
        "size": {
          "0": 220,
          "1": 320
        },
        "flags": {},
        "order": 6,
        "mode": 0,
        "outputs": [
          {
            "name": "IMAGE",
            "type": "IMAGE",
            "links": [21],
            "shape": 3,
            "slot_index": 0
          },
          {
            "name": "MASK",
            "type": "MASK",
            "links": null,
            "shape": 3
          }
        ],
        "properties": {
          "Node name for S&R": "LoadImage"
        },
        "widgets_values": ["ComfyUI_00467_.png", "image"],
        "color": "#223",
        "bgcolor": "#335"
      },
      {
        "id": 17,
        "type": "IPAdapterAdvanced",
        "pos": [651, -104],
        "size": [219.63035311795647, 278],
        "flags": {},
        "order": 9,
        "mode": 0,
        "inputs": [
          {
            "name": "model",
            "type": "MODEL",
            "link": 20
          },
          {
            "name": "ipadapter",
            "type": "IPADAPTER",
            "link": 18
          },
          {
            "name": "image",
            "type": "IMAGE",
            "link": 21
          },
          {
            "name": "image_negative",
            "type": "IMAGE",
            "link": null
          },
          {
            "name": "attn_mask",
            "type": "MASK",
            "link": null
          },
          {
            "name": "clip_vision",
            "type": "CLIP_VISION",
            "link": 19
          }
        ],
        "outputs": [
          {
            "name": "MODEL",
            "type": "MODEL",
            "links": [22],
            "shape": 3,
            "slot_index": 0
          }
        ],
        "properties": {
          "Node name for S&R": "IPAdapterAdvanced"
        },
        "widgets_values": [0.85, "linear", "concat", 0, 1, "V only"]
      }
    ],
    "links": [
      [5, 1, 1, 7, 0, "CLIP"],
      [6, 1, 1, 8, 0, "CLIP"],
      [8, 7, 0, 9, 1, "CONDITIONING"],
      [9, 8, 0, 9, 2, "CONDITIONING"],
      [10, 10, 0, 9, 3, "LATENT"],
      [11, 9, 0, 11, 0, "LATENT"],
      [12, 2, 0, 11, 1, "VAE"],
      [13, 11, 0, 12, 0, "IMAGE"],
      [18, 3, 0, 17, 1, "IPADAPTER"],
      [19, 4, 0, 17, 5, "CLIP_VISION"],
      [20, 1, 0, 17, 0, "MODEL"],
      [21, 6, 0, 17, 2, "IMAGE"],
      [22, 17, 0, 9, 0, "MODEL"]
    ],
    "groups": [],
    "config": {},
    "extra": {
      "ds": {
        "scale": 1,
        "offset": [-110.74409466715406, 315.2359789066653]
      }
    },
    "version": 0.4
  }
}
